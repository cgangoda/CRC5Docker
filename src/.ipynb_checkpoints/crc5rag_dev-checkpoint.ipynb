{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d749a561-c8fc-4991-afe9-7113d3c6017c",
   "metadata": {},
   "source": [
    "### Trying out RAG with ollama and chromadb\n",
    "ollama is installed in the python environment venvcrc5 from where this notebook is started.\n",
    "\n",
    "ollama is recommended over hugging face for local experimentation\n",
    "\n",
    "it uses a docker-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de73eae-777f-4b14-9ed4-cbdd2cfd4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.5.7\n",
      "NAME                       ID              SIZE      MODIFIED     \n",
      "deepseek-r1:latest         0a8c26691023    4.7 GB    22 hours ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    22 hours ago    \n",
      "llama3.1:latest            46e0c10c039e    4.9 GB    9 days ago      \n"
     ]
    }
   ],
   "source": [
    "!ollama --version\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d32e73-ede0-4a86-b35a-7fa55ebf7cfb",
   "metadata": {},
   "source": [
    "#### The pdfreader translates any pdf document to text readable by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59742336-ba28-4ad4-ad9f-b483638d3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "# my textbook, 5th ed\n",
    "reader = PdfReader(\"/home/mort/LaTeX/new projects/CRC5/main.pdf\")\n",
    "total_pages = len(reader.pages)\n",
    "all_text = \"\"\n",
    "for page_num in range(total_pages):\n",
    "    page = reader.pages[page_num]\n",
    "    all_text += page.extract_text()\n",
    "f = open(\"/home/mort/temp/main.txt\", \"w\")\n",
    "f.write(all_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a22089-22a2-4f18-aa53-8c8bc911f7f0",
   "metadata": {},
   "source": [
    "#### Here the original LaTeX files are collected instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d7d2e14-d5c0-4b02-8151-7a63f0031b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Find `.tex` files in LaTeX directory and all subdirectories\n",
    "tex_files = glob.glob('/home/mort/LaTeX/new projects/CRC5/**/chapter[1-9].tex', recursive = True)\n",
    "tex_files.sort()\n",
    "f = open(\"/home/mort/temp/main.txt\", \"w\")\n",
    "for file in tex_files:\n",
    "    g = open(file, \"r\")\n",
    "    content = g.read()\n",
    "    f.write(content)\n",
    "    g.close()\n",
    "f.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ac292-fa28-4fcb-a5dd-002b50a4fc6e",
   "metadata": {},
   "source": [
    "#### Code for preprocessing the RAG supplementary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ad0df7-ee8e-46e9-b238-2399293e17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def readtextfiles(path):\n",
    "    text_contents = {}\n",
    "    directory = os.path.join(path)\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "        \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "        \n",
    "            text_contents[filename] = content\n",
    "        \n",
    "        return text_contents\n",
    "\n",
    "def chunksplitter(text, chunk_size=256, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,  # Desired chunk size in characters or tokens\n",
    "        chunk_overlap=chunk_overlap,  # Overlap between chunks\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]  # Split by paragraphs, then sentences, then words\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# use the nomic-embed-text model to calculate vector embeddings for all text chunks\n",
    "def getembedding(chunks):\n",
    "    embeds = ollama.embed(model=\"nomic-embed-text\", input=chunks)\n",
    "    return embeds.get('embeddings', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f159d-9e67-4f56-af9a-2627d26f042b",
   "metadata": {},
   "source": [
    "#### Add the supplementary text to a new database collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c45bcb2-4e75-413c-83ca-2c36046d094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chromaclient = chromadb.PersistentClient(path=\"/home/mort/crc5imagery/crc5rag\")\n",
    "chromaclient.delete_collection(\"crc5rag\")\n",
    "collection = chromaclient.create_collection(name=\"crc5rag\", metadata={\"hnsw:space\": \"cosine\"}  )\n",
    "\n",
    "# the RAG supplementary data\n",
    "textdocspath = \"/home/mort/temp\"\n",
    "text_data = readtextfiles(textdocspath)\n",
    "\n",
    "# read, break into chunks, embed and add to the chroma vector database \n",
    "for filename, text in text_data.items():\n",
    "    # chunk size set to 256, overlap to 50 (defaults)\n",
    "    chunks = chunksplitter(text)\n",
    "    embeds = getembedding(chunks)\n",
    "    chunknumber = list(range(len(chunks)))\n",
    "    ids = [filename + str(index) for index in chunknumber]\n",
    "    metadatas = [{\"source\": filename} for index in chunknumber]\n",
    "    collection.add(ids=ids, documents=chunks, embeddings=embeds, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9451e-7588-4db5-a87a-d96ee9a152b5",
   "metadata": {},
   "source": [
    "#### Execute a query with llama3.1 or deepseek-r1 and the supplementary text (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66a69bd-b4a6-4c28-91fc-f4b0ff5a1ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 96c415656d37... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \n",
      "pulling 40fb844194b2... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!ollama pull nomic-embed-text\n",
    "#!ollama pull llama3.1\n",
    "!ollama pull deepseek-r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9a7009-0c0d-4566-b29e-fb3a1033dcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "chromaclient = chromadb.PersistentClient(path=\"/home/mort/crc5imagery/crc5rag\")\n",
    "collection = chromaclient.get_collection(name=\"crc5rag\")\n",
    "\n",
    "def ragask(query):\n",
    "    # embed the current query\n",
    "    queryembed = ollama.embed(model=\"nomic-embed-text\", input=query)['embeddings']\n",
    "    # use the embedded current query to retrieve the n_results most relevant document chunks\n",
    "    relateddocs = '\\n\\n'.join(collection.query(query_embeddings=queryembed, n_results=10)['documents'][0])\n",
    "    # generate an answer\n",
    "    prompt = f\"Answer the question: {query}, referring to the following text as a resource: {relateddocs}\"\n",
    "    return ollama.generate(model=\"deepseek-r1\", prompt=prompt, stream=False)['response']\n",
    "\n",
    "gr.Interface(fn=ragask, inputs=\"text\", outputs=\"text\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67440755-d4a5-4ea4-88b1-fcff0231d917",
   "metadata": {},
   "source": [
    "T**Iterated Multivariate Alteration Detection (iMAD): A Step-by-Step Explanation**\n",
    "\n",
    "The iterated multivariate alteration detection (iMAD) algorithm is a statistical method used for change detection between two multispectral images taken at different times. It helps identify changes in land cover, such as deforestation or urban expansion, by analyzing variations in spectral data.\n",
    "\n",
    "**Key Components of iMAD:**\n",
    "\n",
    "1. **Input Images**: The process requires two N-band optical/infrared images capturing the same scene at two distinct time points.\n",
    "\n",
    "2. **Algorithm Purpose**: The goal is to detect changes (alterations) in ground reflectance across multiple image bands, accounting for both radiometric and spectral variations over iterations.\n",
    "\n",
    "3. **Steps in iMAD**:\n",
    "   - **Initialization**: Start with the original images.\n",
    "   - **Iterative Process**: Repeatedly apply a weighted version of Maximum Autocorrelation Factor (MAF) analysis to compute variates that highlight change patterns.\n",
    "   - **Weighting**: In each iteration, certain bands are down-weighted if their contribution to change is deemed significant in prior iterations. This step ensures robust detection by focusing on consistent changes across all bands.\n",
    "\n",
    "4. **Output Variates**: The result includes a set of MAD/MAD-iMAD variates that represent the change patterns between the images.\n",
    "\n",
    "5. **Change Detection**: Variates are thresholded at a specified significance level (e.g., 0.0001). Significant values indicate areas where changes have occurred, while insignificant values show stable regions like built-up or forested areas.\n",
    "\n",
    "**Implementation and Usage:**\n",
    "\n",
    "- **Python Script**: The provided Python script (`iMad.py`) implements the iMAD algorithm using a function that iterates through a specified number of times. Each iteration refines the weighting to enhance change detection accuracy.\n",
    "  \n",
    "- **Example Workflow**: \n",
    "  - Run `scripts/iMad` with input and temporal image paths.\n",
    "  - Use `scripts/iMadmap` for significance thresholding, generating a change map.\n",
    "\n",
    "**Significance:**\n",
    "\n",
    "iMAD is particularly useful in environmental monitoring as it provides detailed insights into change patterns across extensive areas efficiently. By iteratively refining the analysis, iMAD ensures more reliable detection of subtle or persistent changes compared to single-step methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5341c-1900-479e-844b-e23cb38858ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
