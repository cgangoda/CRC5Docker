{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d749a561-c8fc-4991-afe9-7113d3c6017c",
   "metadata": {},
   "source": [
    "### Trying out RAG with ollama and chromadb\n",
    "ollama is installed in the python environment venvcrc5 from where this notebook is started.\n",
    "\n",
    "ollama is recommended over hugging face for local experimentation\n",
    "\n",
    "it uses a docker-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c5d425-1265-4a48-9797-f0813083b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy UI\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de73eae-777f-4b14-9ed4-cbdd2cfd4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language model runner\n",
      "\n",
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  stop        Stop a running model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "!ollama --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80d6cb-3bd8-487b-997f-49e615211cdd",
   "metadata": {},
   "source": [
    "#### This model is required for embedding the additional information to the supplied query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ceaf8f-b33a-4af7-bfb3-8ac144622d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483b25c-cb2c-4eca-92ea-52e66d5f28ab",
   "metadata": {},
   "source": [
    "#### This is one of the basic ollama LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444fc411-90a5-40df-aa93-a62e80607a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b38620b-dd63-4980-9b01-7390f3c98acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED   \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    5 days ago    \n",
      "llama3.1:latest            46e0c10c039e    4.9 GB    6 days ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ed021-f6cd-4759-a5a2-973447d22431",
   "metadata": {},
   "source": [
    "#### Run a simple query with the llama3.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b200a7-999d-4b6f-9ac6-ca25fa2c1a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask(question):\n",
    "    response: ChatResponse = chat(model='llama3.1', messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': str(question),\n",
    "      },\n",
    "    ])\n",
    "    return response.message.content \n",
    "\n",
    "gr.Interface(fn=ask, inputs=\"text\", outputs=\"text\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d32e73-ede0-4a86-b35a-7fa55ebf7cfb",
   "metadata": {},
   "source": [
    "#### The pdfreader translates any pdf document to text readable by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59742336-ba28-4ad4-ad9f-b483638d3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "# my textbook, 5th ed\n",
    "reader = PdfReader(\"/home/mort/LaTeX/new projects/CRC5/main.pdf\")\n",
    "total_pages = len(reader.pages)\n",
    "all_text = \"\"\n",
    "for page_num in range(total_pages):\n",
    "    page = reader.pages[page_num]\n",
    "    all_text += page.extract_text()\n",
    "f = open(\"/home/mort/temp/main.txt\", \"w\")\n",
    "f.write(all_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d2468-a48f-44c6-bef4-c8e489d5daf0",
   "metadata": {},
   "source": [
    "#### This starts a chroma embedding database as a docker container with entrypoint on port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e60615e-1f18-4968-b50b-6a0adcb5914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY        TAG       IMAGE ID       CREATED       SIZE\n",
      "chromadb/chroma   latest    1295eb7aaaed   2 weeks ago   469MB\n",
      "mort/crc5docker   latest    9f6ec881b8c5   6 weeks ago   5GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2056d10-8d00-4719-aa11-fd2a83662012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb\n"
     ]
    }
   ],
   "source": [
    "!docker start chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2944e2-fb59-4f99-9ec3-11db8f156796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE             COMMAND                  CREATED       STATUS         PORTS                                       NAMES\n",
      "47137d05119e   chromadb/chroma   \"/docker_entrypoint.…\"   2 weeks ago   Up 7 seconds   0.0.0.0:8000->8000/tcp, :::8000->8000/tcp   chromadb\n"
     ]
    }
   ],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ac292-fa28-4fcb-a5dd-002b50a4fc6e",
   "metadata": {},
   "source": [
    "#### Code for preprocessing the RAG supplementary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ad0df7-ee8e-46e9-b238-2399293e17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ollama\n",
    "\n",
    "def readtextfiles(path):\n",
    "  text_contents = {}\n",
    "  directory = os.path.join(path)\n",
    "\n",
    "  for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "      file_path = os.path.join(directory, filename)\n",
    "\n",
    "      with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "      text_contents[filename] = content\n",
    "\n",
    "  return text_contents\n",
    "\n",
    "# split text into equal size chunks\n",
    "def chunksplitter(text, chunk_size=100):\n",
    "  words = re.findall(r'\\S+', text)\n",
    "\n",
    "  chunks = []\n",
    "  current_chunk = []\n",
    "  word_count = 0\n",
    "\n",
    "  for word in words:\n",
    "    current_chunk.append(word)\n",
    "    word_count += 1\n",
    "\n",
    "    if word_count >= chunk_size:\n",
    "      chunks.append(' '.join(current_chunk))\n",
    "      current_chunk = []\n",
    "      word_count = 0\n",
    "\n",
    "  if current_chunk:\n",
    "    chunks.append(' '.join(current_chunk))\n",
    "\n",
    "  return chunks\n",
    "\n",
    "# use the nomic-embed-text model to calculate vector embeddings for all text chunks\n",
    "def getembedding(chunks):\n",
    "  embeds = ollama.embed(model=\"nomic-embed-text\", input=chunks)\n",
    "  return embeds.get('embeddings', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88ecf52-8c8a-4b43-adc9-564521cb7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# use REST API for chroma vector database\n",
    "chromaclient = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f159d-9e67-4f56-af9a-2627d26f042b",
   "metadata": {},
   "source": [
    "#### Add the supplementary text to a new database collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c45bcb2-4e75-413c-83ca-2c36046d094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erase any existing collection and create a new empty one\n",
    "chromaclient.delete_collection(name=\"ragwithpython\")\n",
    "collection = chromaclient.get_or_create_collection(name=\"ragwithpython\", metadata={\"hnsw:space\": \"cosine\"}  )\n",
    "\n",
    "# the RAG supplementary data\n",
    "textdocspath = \"/home/mort/temp\"\n",
    "text_data = readtextfiles(textdocspath)\n",
    "\n",
    "# read, break into chunks, embed and add to the chroma vector database \n",
    "for filename, text in text_data.items():\n",
    "  # chunk size 256\n",
    "  chunks = chunksplitter(text, 256)\n",
    "  embeds = getembedding(chunks)\n",
    "  chunknumber = list(range(len(chunks)))\n",
    "  ids = [filename + str(index) for index in chunknumber]\n",
    "  metadatas = [{\"source\": filename} for index in chunknumber]\n",
    "  collection.add(ids=ids, documents=chunks, embeddings=embeds, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9451e-7588-4db5-a87a-d96ee9a152b5",
   "metadata": {},
   "source": [
    "#### Execute a query with the supplementary text (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae9a7009-0c0d-4566-b29e-fb3a1033dcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio/flagged/dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "collection = chromaclient.get_or_create_collection(name=\"ragwithpython\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "def ragask(query):\n",
    "    # embed the current query\n",
    "    queryembed = ollama.embed(model=\"nomic-embed-text\", input=query)['embeddings']\n",
    "    \n",
    "    # use the embedded current query to retrieve the most relevant document chunks (as text NOT as embeddings)\n",
    "    relateddocs = '\\n\\n'.join(collection.query(query_embeddings=queryembed, n_results=4)['documents'][0])\n",
    "    \n",
    "    # generate an answer\n",
    "    prompt = f\"Answer the question: {query} referring to the following text as a resource: {relateddocs}\"\n",
    "    ragoutput = ollama.generate(model=\"llama3.1\", prompt=prompt, stream=False)\n",
    "\n",
    "    return ragoutput['response']\n",
    "\n",
    "# use the gradio interface\n",
    "gr.Interface(fn=ragask, inputs=\"text\", outputs=\"text\").launch()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead031c-4ce1-43a7-bf2c-2669ceabb755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
