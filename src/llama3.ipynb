{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d749a561-c8fc-4991-afe9-7113d3c6017c",
   "metadata": {},
   "source": [
    "### Trying out RAG with ollama and chromadb\n",
    "ollama is installed in the python environment venvcrc5 from where this notebook is started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de73eae-777f-4b14-9ed4-cbdd2cfd4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language model runner\n",
      "\n",
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  stop        Stop a running model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "!ollama --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80d6cb-3bd8-487b-997f-49e615211cdd",
   "metadata": {},
   "source": [
    "#### This model is required for embedding the additional information to the supplied query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ceaf8f-b33a-4af7-bfb3-8ac144622d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483b25c-cb2c-4eca-92ea-52e66d5f28ab",
   "metadata": {},
   "source": [
    "#### This is one of the basic ollama LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444fc411-90a5-40df-aa93-a62e80607a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 667b0c1932bc... 100% ▕████████████████▏ 4.9 GB                         \n",
      "pulling 948af2743fc7... 100% ▕████████████████▏ 1.5 KB                         \n",
      "pulling 0ba8f0e314b4... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \n",
      "pulling 455f34728c9b... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7dfd8a8-7280-439a-8f11-3fa16f265bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?25l\u001b[?25h\u001b[2K\u001b[1G\u001b[?25hdeleted 'llama3.2'\n"
     ]
    }
   ],
   "source": [
    "!ollama rm llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b38620b-dd63-4980-9b01-7390f3c98acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED     \n",
      "llama3.1:latest            46e0c10c039e    4.9 GB    23 hours ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    23 hours ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c152fbd3-4d7d-4393-b44c-48a746c206cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b200a7-999d-4b6f-9ac6-ca25fa2c1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    response: ChatResponse = chat(model='llama3.1', messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': str(question),\n",
    "      },\n",
    "    ])\n",
    "    print(response.message.content)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e251006-ecc4-49fa-967c-92e9ec1bddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for Retrieval Augmented Generation. It's a type of artificial intelligence model architecture that combines the strengths of retrieval-based models with those of generative models.\n",
      "\n",
      "**Retrieval-based models** are designed to retrieve relevant information from a large database or knowledge graph based on user input. They're typically used for question answering, natural language processing, and other tasks where retrieving existing knowledge is sufficient.\n",
      "\n",
      "**Generative models**, on the other hand, generate new content, such as text, images, or code, based on patterns learned from existing data. They can create novel outputs that don't exist in the training dataset.\n",
      "\n",
      "RAG combines these two approaches by augmenting a generative model with retrieval capabilities. Here's how it works:\n",
      "\n",
      "1. **Retrieval**: The RAG model first searches a large database (e.g., a knowledge graph) to retrieve relevant information related to the user input.\n",
      "2. **Augmentation**: This retrieved information is then used to condition or inform the generation process of the underlying generative model.\n",
      "3. **Generation**: The generative model generates output based on the augmented input, using both the user's query and the retrieved knowledge.\n",
      "\n",
      "RAG has several benefits:\n",
      "\n",
      "1. **Improved accuracy**: By incorporating external knowledge, RAG can generate more accurate and informative responses to complex queries.\n",
      "2. **Increased efficiency**: Retrieval can reduce the computational cost of generating new content by leveraging existing information.\n",
      "3. **Enhanced robustness**: RAG models are less prone to hallucinations (i.e., generating nonsensical or entirely made-up text) since they're informed by actual knowledge.\n",
      "\n",
      "RAG is widely used in applications like:\n",
      "\n",
      "1. Conversational AI\n",
      "2. Question answering\n",
      "3. Text generation (e.g., summarization, chatbots)\n",
      "4. Knowledge graph construction\n",
      "\n",
      "The concept of RAG has gained significant attention in the research community and industry, as it offers a promising approach for building more robust and accurate language models.\n"
     ]
    }
   ],
   "source": [
    "ask('What is RAG (retrieval augmented generation)?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d32e73-ede0-4a86-b35a-7fa55ebf7cfb",
   "metadata": {},
   "source": [
    "#### The pdfreader translates any pdf document to text readable by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59742336-ba28-4ad4-ad9f-b483638d3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my entire book\n",
    "reader = PdfReader(\"/home/mort/LaTeX/new projects/remotesensing2019/draft.pdf\")\n",
    "total_pages = len(reader.pages)\n",
    "all_text = \"\"\n",
    "for page_num in range(total_pages):\n",
    "    page = reader.pages[page_num]\n",
    "    all_text += page.extract_text()\n",
    "#print(all_text)\n",
    "f = open(\"/home/mort/temp/main.txt\", \"w\")\n",
    "f.write(all_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d2468-a48f-44c6-bef4-c8e489d5daf0",
   "metadata": {},
   "source": [
    "#### This starts a chroma embedding database as a docker container with entrypoint on port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e60615e-1f18-4968-b50b-6a0adcb5914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY        TAG       IMAGE ID       CREATED       SIZE\n",
      "chromadb/chroma   latest    1295eb7aaaed   6 days ago    469MB\n",
      "mort/crc5docker   latest    9f6ec881b8c5   4 weeks ago   5GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2944e2-fb59-4f99-9ec3-11db8f156796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE             COMMAND                  CREATED      STATUS                    PORTS     NAMES\n",
      "47137d05119e   chromadb/chroma   \"/docker_entrypoint.…\"   2 days ago   Exited (0) 13 hours ago             chromadb\n"
     ]
    }
   ],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2056d10-8d00-4719-aa11-fd2a83662012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb\n"
     ]
    }
   ],
   "source": [
    "!docker start chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ac292-fa28-4fcb-a5dd-002b50a4fc6e",
   "metadata": {},
   "source": [
    "#### Code for preprocessing the RAG supplementary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ad0df7-ee8e-46e9-b238-2399293e17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def readtextfiles(path):\n",
    "  text_contents = {}\n",
    "  directory = os.path.join(path)\n",
    "\n",
    "  for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "      file_path = os.path.join(directory, filename)\n",
    "\n",
    "      with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "      text_contents[filename] = content\n",
    "\n",
    "  return text_contents\n",
    "\n",
    "def chunksplitter(text, chunk_size=100):\n",
    "  words = re.findall(r'\\S+', text)\n",
    "\n",
    "  chunks = []\n",
    "  current_chunk = []\n",
    "  word_count = 0\n",
    "\n",
    "  for word in words:\n",
    "    current_chunk.append(word)\n",
    "    word_count += 1\n",
    "\n",
    "    if word_count >= chunk_size:\n",
    "      chunks.append(' '.join(current_chunk))\n",
    "      current_chunk = []\n",
    "      word_count = 0\n",
    "\n",
    "  if current_chunk:\n",
    "    chunks.append(' '.join(current_chunk))\n",
    "\n",
    "  return chunks\n",
    "\n",
    "def getembedding(chunks):\n",
    "  embeds = ollama.embed(model=\"nomic-embed-text\", input=chunks)\n",
    "  return embeds.get('embeddings', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88ecf52-8c8a-4b43-adc9-564521cb7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chromaclient = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000\n",
    ")\n",
    "\n",
    "collection = chromaclient.get_or_create_collection(name=\"ragwithpython\", metadata={\"hnsw:space\": \"cosine\"}  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f159d-9e67-4f56-af9a-2627d26f042b",
   "metadata": {},
   "source": [
    "#### Add the supplementary text to a database collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c45bcb2-4e75-413c-83ca-2c36046d094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# *run only to erase existing collection 'ragwithpython'*\n",
    "chromaclient.delete_collection(name=\"ragwithpython\")\n",
    "collection = chromaclient.get_or_create_collection(name=\"ragwithpython\", metadata={\"hnsw:space\": \"cosine\"}  )\n",
    "\n",
    "textdocspath = \"/home/mort/temp\"\n",
    "text_data = readtextfiles(textdocspath)\n",
    "\n",
    "for filename, text in text_data.items():\n",
    "  # chunk size 100\n",
    "  chunks = chunksplitter(text, 100)\n",
    "  embeds = getembedding(chunks)\n",
    "  chunknumber = list(range(len(chunks)))\n",
    "  ids = [filename + str(index) for index in chunknumber]\n",
    "  metadatas = [{\"source\": filename} for index in chunknumber]\n",
    "  collection.add(ids=ids, documents=chunks, embeddings=embeds, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9451e-7588-4db5-a87a-d96ee9a152b5",
   "metadata": {},
   "source": [
    "#### Execute a query with the supplementary text (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6bb63f1-670e-4c2c-ab7e-36333489509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answered with RAG: This text appears to be a research article related to remote sensing and Earth observation using the Google Earth Engine (GEE). Here's a summary of the main points:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "* The article discusses the use of Sentinel-1 data from the GEE for change detection tasks.\n",
      "* The authors highlight the advantages of Sentinel-1, including its high spatial resolution (up to 20 meters), short revisit times (every 6 days), and independence from solar illumination and cloud cover.\n",
      "\n",
      "**Background**\n",
      "\n",
      "* The article mentions that the GEE provides a powerful platform for processing and analyzing large datasets.\n",
      "* However, it notes that only dual polarization multi-look intensity format data are available in the GEE archive, which limits the use of interferometric coherence methods.\n",
      "\n",
      "**Methodology**\n",
      "\n",
      "* The authors describe a sequential omnibus algorithm for change detection, which is based on complex Wishart statistics.\n",
      "* The algorithm is designed to identify significant changes in covariance matrices over time and can detect multiple changes at the same location.\n",
      "\n",
      "**Results**\n",
      "\n",
      "* The article presents a color-coded change map of the total number of significant changes in a sequence of 74 Sentinel-1 images over the NATO Airbase near Geilenkirchen, Germany.\n",
      "* The results show that the algorithm can pinpoint regions of high anthropogenic activity, such as port activities and uranium mining.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "* The article concludes that the sequential omnibus algorithm is effective for change detection tasks using Sentinel-1 data from the GEE.\n",
      "* It highlights the potential applications of this method in various fields, including flood monitoring, deforestation, and port activity tracking.\n",
      "\n",
      "Some possible research questions or extensions based on this text could be:\n",
      "\n",
      "* Can the algorithm be adapted to work with other types of satellite data?\n",
      "* How does the performance of the algorithm vary depending on the spatial resolution of the input data?\n",
      "* What are some potential applications of this method in fields such as environmental monitoring or urban planning?\n",
      "\n",
      "Note that the article appears to be a preprint, and it may have undergone revisions before publication.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "query = \"how does sequential sar change detection work?\"\n",
    "\n",
    "queryembed = ollama.embed(model=\"nomic-embed-text\", input=query)['embeddings']\n",
    "\n",
    "relateddocs = '\\n\\n'.join(collection.query(query_embeddings=queryembed, n_results=20)['documents'][0])\n",
    "\n",
    "prompt = f\"{query} - Answer that question using the following text as a resource: {relateddocs}\"\n",
    "ragoutput = ollama.generate(model=\"llama3.1\", prompt=prompt, stream=False)\n",
    "\n",
    "print(f\"Answered with RAG: {ragoutput['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c283af62-167a-4203-be61-3ca2752f175a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
