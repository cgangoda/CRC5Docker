{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d749a561-c8fc-4991-afe9-7113d3c6017c",
   "metadata": {},
   "source": [
    "### Trying out RAG with ollama and chromadb\n",
    "ollama is installed in the python environment venvcrc5 from where this notebook is started.\n",
    "\n",
    "ollama is recommended over hugging face for local experimentation\n",
    "\n",
    "it uses a docker-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de73eae-777f-4b14-9ed4-cbdd2cfd4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.5.7\n",
      "NAME                       ID              SIZE      MODIFIED     \n",
      "deepseek-r1:latest         0a8c26691023    4.7 GB    45 hours ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    45 hours ago    \n",
      "llama3.1:latest            46e0c10c039e    4.9 GB    4 days ago      \n"
     ]
    }
   ],
   "source": [
    "!ollama --version\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d32e73-ede0-4a86-b35a-7fa55ebf7cfb",
   "metadata": {},
   "source": [
    "#### The pdfreader translates any pdf document to text readable by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59742336-ba28-4ad4-ad9f-b483638d3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "# my textbook, 5th ed\n",
    "reader = PdfReader(\"/home/mort/LaTeX/new projects/CRC5/main.pdf\")\n",
    "total_pages = len(reader.pages)\n",
    "all_text = \"\"\n",
    "for page_num in range(total_pages):\n",
    "    page = reader.pages[page_num]\n",
    "    all_text += page.extract_text()\n",
    "f = open(\"/home/mort/temp/main.txt\", \"w\")\n",
    "f.write(all_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ac292-fa28-4fcb-a5dd-002b50a4fc6e",
   "metadata": {},
   "source": [
    "#### Code for preprocessing the RAG supplementary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ad0df7-ee8e-46e9-b238-2399293e17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def readtextfiles(path):\n",
    "    text_contents = {}\n",
    "    directory = os.path.join(path)\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "        \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "        \n",
    "            text_contents[filename] = content\n",
    "        \n",
    "        return text_contents\n",
    "\n",
    "def chunksplitter(text, chunk_size=256, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,  # Desired chunk size in characters or tokens\n",
    "        chunk_overlap=chunk_overlap,  # Overlap between chunks\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]  # Split by paragraphs, then sentences, then words\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# use the nomic-embed-text model to calculate vector embeddings for all text chunks\n",
    "def getembedding(chunks):\n",
    "    embeds = ollama.embed(model=\"nomic-embed-text\", input=chunks)\n",
    "    return embeds.get('embeddings', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f159d-9e67-4f56-af9a-2627d26f042b",
   "metadata": {},
   "source": [
    "#### Add the supplementary text to a new database collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c45bcb2-4e75-413c-83ca-2c36046d094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chromaclient = chromadb.PersistentClient(path=\"/home/mort/crc5imagery/crc5rag\")\n",
    "# erase any existing collection and create a new empty one\n",
    "chromaclient.delete_collection(name=\"crc5rag\")\n",
    "collection = chromaclient.create_collection(name=\"crc5rag\", metadata={\"hnsw:space\": \"cosine\"}  )\n",
    "\n",
    "# the RAG supplementary data\n",
    "textdocspath = \"/home/mort/temp\"\n",
    "text_data = readtextfiles(textdocspath)\n",
    "\n",
    "# read, break into chunks, embed and add to the chroma vector database \n",
    "for filename, text in text_data.items():\n",
    "    # chunk size default 256, overlap default 50\n",
    "    chunks = chunksplitter(text,512,100)\n",
    "    embeds = getembedding(chunks)\n",
    "    chunknumber = list(range(len(chunks)))\n",
    "    ids = [filename + str(index) for index in chunknumber]\n",
    "    metadatas = [{\"source\": filename} for index in chunknumber]\n",
    "    collection.add(ids=ids, documents=chunks, embeddings=embeds, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9451e-7588-4db5-a87a-d96ee9a152b5",
   "metadata": {},
   "source": [
    "#### Execute a query with llama3.1 or deepseek-r1 and the supplementary text (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66a69bd-b4a6-4c28-91fc-f4b0ff5a1ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 96c415656d37... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \n",
      "pulling 40fb844194b2... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!ollama pull nomic-embed-text\n",
    "#!ollama pull llama3.1\n",
    "!ollama pull deepseek-r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9a7009-0c0d-4566-b29e-fb3a1033dcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "chromaclient = chromadb.PersistentClient(path=\"/home/mort/crc5imagery/crc5rag\")\n",
    "collection = chromaclient.get_collection(name=\"crc5rag\")\n",
    "\n",
    "def ragask(query):\n",
    "    # embed the current query\n",
    "    queryembed = ollama.embed(model=\"nomic-embed-text\", input=query)['embeddings']\n",
    "    \n",
    "    # use the embedded current query to retrieve the most relevant document chunks (as text NOT as embeddings)\n",
    "    relateddocs = '\\n\\n'.join(collection.query(query_embeddings=queryembed, n_results=4)['documents'][0])\n",
    "\n",
    "    # generate an answer\n",
    "    prompt = f\"Answer the question: {query}, referring to the following text as a resource: {relateddocs}\"\n",
    "    ragoutput = ollama.generate(model=\"deepseek-r1\", prompt=prompt, stream=False)\n",
    "\n",
    "    return ragoutput['response']\n",
    "\n",
    "# use the gradio interface\n",
    "gr.Interface(fn=ragask, inputs=\"text\", outputs=\"text\").launch()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19351b2c-d8ad-4fe3-8cf3-494dd00905cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
